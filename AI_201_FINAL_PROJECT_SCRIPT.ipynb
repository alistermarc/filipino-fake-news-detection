{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8716de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:25:28.963311Z",
     "start_time": "2023-12-30T18:25:27.399412Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab9317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:32:26.848263Z",
     "start_time": "2023-12-30T18:25:28.966306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for MultinomialNB: 0.8598130841121495\n",
      "Training Accuracy Score for MultinomialNB: 0.8685664900097466\n",
      "Training Accuracy all training data for MultinomialNB: 0.9602184087363494\n",
      "\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for LogisticRegression: 0.9485981308411215\n",
      "Training Accuracy Score for LogisticRegression: 0.9356420565302145\n",
      "Training Accuracy all training data for LogisticRegression: 0.9781591263650546\n",
      "\n",
      "\n",
      "RandomForestClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for RandomForestClassifier: 0.940809968847352\n",
      "Training Accuracy Score for RandomForestClassifier: 0.9411024305555555\n",
      "Training Accuracy all training data for RandomForestClassifier: 1.0\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for DecisionTreeClassifier: 0.9065420560747663\n",
      "Training Accuracy Score for DecisionTreeClassifier: 0.9130238791423002\n",
      "Training Accuracy all training data for DecisionTreeClassifier: 1.0\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for GradientBoostingClassifier: 0.9376947040498442\n",
      "Training Accuracy Score for GradientBoostingClassifier: 0.9290181834795321\n",
      "Training Accuracy all training data for GradientBoostingClassifier: 0.9652886115444618\n",
      "\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for AdaBoostClassifier: 0.940809968847352\n",
      "Training Accuracy Score for AdaBoostClassifier: 0.9340810794346979\n",
      "Training Accuracy all training data for AdaBoostClassifier: 0.9613884555382215\n",
      "\n",
      "\n",
      "SVC(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for SVC: 0.956386292834891\n",
      "Training Accuracy Score for SVC: 0.9450041118421051\n",
      "Training Accuracy all training data for SVC: 1.0\n",
      "\n",
      "\n",
      "KNeighborsClassifier()\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for KNeighborsClassifier: 0.8785046728971962\n",
      "Training Accuracy Score for KNeighborsClassifier: 0.8689555921052632\n",
      "Training Accuracy all training data for KNeighborsClassifier: 0.9274570982839313\n",
      "\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for SGDClassifier: 0.9626168224299065\n",
      "Training Accuracy Score for SGDClassifier: 0.9578787463450291\n",
      "Training Accuracy all training data for SGDClassifier: 1.0\n",
      "\n",
      "\n",
      "MLPClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Test Accuracy for MLPClassifier: 0.9610591900311527\n",
      "Training Accuracy Score for MLPClassifier: 0.9512487816764132\n",
      "Training Accuracy all training data for MLPClassifier: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Tagalog stop words\n",
    "with open('tagalog_stop_words.txt', 'r', encoding='utf-8') as file:\n",
    "    stop_words = [line.strip() for line in file]\n",
    "\n",
    "# Load the Tagalog dataset\n",
    "df = pd.read_csv('Dataset/full.csv', encoding='utf-8')\n",
    "\n",
    "# Assuming your dataset has 'text' and 'label' columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['article'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define classifiers to compare, including AdaBoostClassifier\n",
    "random_state = 42  # Set a fixed random state for reproducibility\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=random_state),\n",
    "    RandomForestClassifier(random_state=random_state),\n",
    "    DecisionTreeClassifier(random_state=random_state),\n",
    "    GradientBoostingClassifier(random_state=random_state),\n",
    "    AdaBoostClassifier(random_state=random_state),\n",
    "    SVC(random_state=random_state),\n",
    "    KNeighborsClassifier(),\n",
    "    SGDClassifier(random_state=random_state),\n",
    "    MLPClassifier(random_state=random_state)\n",
    "]\n",
    "test_results_list = []\n",
    "train_accuracy_list = []  # List to store training accuracy values\n",
    "models = []\n",
    "joblib_path = r'C:\\Users\\LEGION\\UMS Group - GBIC Dropbox\\Jemar Laag\\ACADS\\AI 201\\Fake News Detection\\No Hyperparameterer tuning/'\n",
    "train_results_list = []\n",
    "for classifier in classifiers:\n",
    "    print(classifier)\n",
    "    \n",
    "    # Hyperparameter grids for TfidfVectorizer and classifiers\n",
    "    param_grids = {\n",
    "        'tfidf__max_df': [0.9],  # Example max_df values\n",
    "    }\n",
    "\n",
    "    classifier_name = type(classifier).__name__.lower()\n",
    "\n",
    "    # Set the classifier in the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stop_words,lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    start_time = time.time()\n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grids, scoring='accuracy', cv=5, n_jobs=-1, verbose=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time = time.time()  # Record the end time\n",
    "    total_training_time = end_time - start_time \n",
    "    # Save the best model using joblib\n",
    "    best_model = grid_search.best_estimator_\n",
    "    model_filename = f\"{joblib_path}{type(classifier).__name__}_best_model.joblib\"\n",
    "    dump(best_model, model_filename)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy_score = grid_search.best_score_\n",
    "    train_accuracy_list.append({\n",
    "        'Classifier': type(classifier).__name__,\n",
    "        'Train Accuracy': train_accuracy_score,\n",
    "        'Total Training Time (s)': total_training_time\n",
    "    })\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = grid_search.predict(X_test)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    confusion = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # Append results to the list\n",
    "    test_results_list.append({\n",
    "        'Classifier': type(classifier).__name__,\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'Specificity': specificity,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "    \n",
    "    train_predictions = grid_search.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    train_confusion = confusion_matrix(y_train, train_predictions)\n",
    "    train_tn, train_fp, train_fn, train_tp = train_confusion.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_recall = train_tp / (train_tp + train_fn)\n",
    "    train_precision = train_tp / (train_tp + train_fp)\n",
    "    train_specificity = train_tn / (train_tn + train_fp)\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    # Append results to the list\n",
    "    train_results_list.append({\n",
    "        'Classifier': type(classifier).__name__,\n",
    "        'Accuracy': train_accuracy,\n",
    "        'Recall':train_recall,\n",
    "        'Precision': train_precision,\n",
    "        'F1 Score': train_f1,\n",
    "        'Specificity': train_specificity,\n",
    "        'TP': train_tp,\n",
    "        'TN': train_tn,\n",
    "        'FP': train_fp,\n",
    "        'FN': train_fn\n",
    "    })\n",
    "    \n",
    "    \n",
    "    print(f\"Test Accuracy for {type(classifier).__name__}: {accuracy}\")\n",
    "    print(f\"Training Accuracy Score for {type(classifier).__name__}: {train_accuracy_score}\")\n",
    "    print(f\"Training Accuracy all training data for {type(classifier).__name__}: {train_accuracy}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Convert the lists of dictionaries to DataFrames\n",
    "test_results_df = pd.DataFrame(test_results_list)\n",
    "train_accuracy_df = pd.DataFrame(train_accuracy_list)\n",
    "train_accuracy_df_all  = pd.DataFrame(train_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668a89dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T18:32:26.869200Z",
     "start_time": "2023-12-30T18:32:26.851249Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results_df.to_csv('TEST_ACCURACY_DEFINED_HYPERPARAMETER.csv')\n",
    "train_accuracy_df.to_csv('TRAIN_ACCURACY_DEFINED_HYPERPARAMETER.csv')\n",
    "train_accuracy_df_all.to_csv('TRAIN_ACCURACY_OVERALL_TRAIN_DATA_DEFINED_HYPERPARAMETER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20e8a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T23:10:08.748949Z",
     "start_time": "2023-12-30T18:32:26.872192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Test Accuracy for MultinomialNB: 0.9174454828660437\n",
      "Training Accuracy Score for MultinomialNB: 0.9181004812378166\n",
      "Training Accuracy all training data for MultinomialNB: 0.9910296411856474\n",
      "Best Parameters for MultinomialNB: {'classifier__alpha': 0.1, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "LogisticRegression(random_state=42)\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Test Accuracy for LogisticRegression: 0.9595015576323987\n",
      "Training Accuracy Score for LogisticRegression: 0.952416087962963\n",
      "Training Accuracy all training data for LogisticRegression: 1.0\n",
      "Best Parameters for LogisticRegression: {'classifier__C': 10.0, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "RandomForestClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Test Accuracy for RandomForestClassifier: 0.9485981308411215\n",
      "Training Accuracy Score for RandomForestClassifier: 0.9418844420077972\n",
      "Training Accuracy all training data for RandomForestClassifier: 1.0\n",
      "Best Parameters for RandomForestClassifier: {'classifier__n_estimators': 200, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "Test Accuracy for DecisionTreeClassifier: 0.9080996884735203\n",
      "Training Accuracy Score for DecisionTreeClassifier: 0.9141972770467837\n",
      "Training Accuracy all training data for DecisionTreeClassifier: 0.9863494539781591\n",
      "Best Parameters for DecisionTreeClassifier: {'classifier__max_depth': 20, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Test Accuracy for GradientBoostingClassifier: 0.9376947040498442\n",
      "Training Accuracy Score for GradientBoostingClassifier: 0.9360380116959064\n",
      "Training Accuracy all training data for GradientBoostingClassifier: 0.9941497659906396\n",
      "Best Parameters for GradientBoostingClassifier: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "AdaBoostClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Test Accuracy for AdaBoostClassifier: 0.9361370716510904\n",
      "Training Accuracy Score for AdaBoostClassifier: 0.9430547941033138\n",
      "Training Accuracy all training data for AdaBoostClassifier: 1.0\n",
      "Best Parameters for AdaBoostClassifier: {'classifier__learning_rate': 1.0, 'classifier__n_estimators': 200, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "SVC(random_state=42)\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Test Accuracy for SVC: 0.9610591900311527\n",
      "Training Accuracy Score for SVC: 0.9570982577972711\n",
      "Training Accuracy all training data for SVC: 1.0\n",
      "Best Parameters for SVC: {'classifier__C': 10.0, 'classifier__kernel': 'linear', 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "KNeighborsClassifier()\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Test Accuracy for KNeighborsClassifier: 0.8722741433021807\n",
      "Training Accuracy Score for KNeighborsClassifier: 0.88299448708577\n",
      "Training Accuracy all training data for KNeighborsClassifier: 1.0\n",
      "Best Parameters for KNeighborsClassifier: {'classifier__n_neighbors': 7, 'classifier__weights': 'distance', 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "\n",
      "SGDClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Test Accuracy for SGDClassifier: 0.9626168224299065\n",
      "Training Accuracy Score for SGDClassifier: 0.9578787463450291\n",
      "Training Accuracy all training data for SGDClassifier: 1.0\n",
      "Best Parameters for SGDClassifier: {'classifier__alpha': 0.0001, 'classifier__penalty': 'l2', 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "MLPClassifier(random_state=42)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Test Accuracy for MLPClassifier: 0.9610591900311527\n",
      "Training Accuracy Score for MLPClassifier: 0.9512487816764132\n",
      "Training Accuracy all training data for MLPClassifier: 1.0\n",
      "Best Parameters for MLPClassifier: {'classifier__alpha': 0.0001, 'classifier__learning_rate_init': 0.001, 'tfidf__max_df': 0.7, 'tfidf__ngram_range': (1, 1)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_results_list_tuned = []\n",
    "train_accuracy_list_tuned= []  # List to store training accuracy values\n",
    "models = []\n",
    "joblib_path = r'C:\\Users\\LEGION\\UMS Group - GBIC Dropbox\\Jemar Laag\\ACADS\\AI 201\\Fake News Detection\\With Hyperparameter tuning/'\n",
    "train_results_list_tuned = []\n",
    "for classifier in classifiers:\n",
    "    print(classifier)\n",
    "    \n",
    "    # Hyperparameter grids for TfidfVectorizer and classifiers\n",
    "    param_grids = {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],  # Example ngram_range values\n",
    "        'tfidf__max_df': [ 0.7, 0.8, 0.9],  # Example max_df values\n",
    "    }\n",
    "\n",
    "    classifier_name = type(classifier).__name__.lower()\n",
    "    if isinstance(classifier, MultinomialNB):\n",
    "        param_grids[f'classifier__alpha'] = [0.1, 0.5, 1.0]\n",
    "        \n",
    "    elif isinstance(classifier, LogisticRegression):\n",
    "        param_grids[f'classifier__C'] = [0.1, 1.0, 10.0]\n",
    "    elif isinstance(classifier, RandomForestClassifier):\n",
    "        param_grids[f'classifier__n_estimators'] = [50, 100, 200]\n",
    "    elif isinstance(classifier, MLPClassifier):\n",
    "        #param_grids[f'classifier__hidden_layer_sizes'] = [(50,)]\n",
    "        param_grids[f'classifier__alpha'] = [0.0001, 0.001]\n",
    "        #param_grids[f'classifier__activation'] = ['relu']\n",
    "        param_grids[f'classifier__learning_rate_init'] = [0.001, 0.1]\n",
    "    elif isinstance(classifier, AdaBoostClassifier):\n",
    "        param_grids[f'classifier__n_estimators'] = [50, 100, 200]\n",
    "        param_grids[f'classifier__learning_rate'] = [0.01, 0.1, 1.0]\n",
    "    elif isinstance(classifier, SVC):\n",
    "        param_grids[f'classifier__C'] = [0.1, 1.0, 10.0]\n",
    "        param_grids[f'classifier__kernel'] = ['linear', 'rbf']\n",
    "    elif isinstance(classifier, KNeighborsClassifier):\n",
    "        param_grids[f'classifier__n_neighbors'] = [3, 5, 7]\n",
    "        param_grids[f'classifier__weights'] = ['uniform', 'distance']\n",
    "    elif isinstance(classifier, SGDClassifier):\n",
    "        param_grids[f'classifier__alpha'] = [0.0001, 0.001]\n",
    "        param_grids[f'classifier__penalty'] = ['l2', 'l1', 'elasticnet']\n",
    "    elif isinstance(classifier, DecisionTreeClassifier):\n",
    "        param_grids[f'classifier__max_depth'] = [None, 10,20]\n",
    "        param_grids[f'classifier__min_samples_split'] = [2, 5]\n",
    "        param_grids[f'classifier__min_samples_leaf'] = [1, 2]\n",
    "\n",
    "    # Add hyperparameters for GradientBoostingClassifier\n",
    "    elif isinstance(classifier, GradientBoostingClassifier):\n",
    "        param_grids[f'classifier__n_estimators'] = [50, 100]  # Reduced number of values\n",
    "        param_grids[f'classifier__learning_rate'] = [0.01, 0.1]  # Reduced number of values\n",
    "        param_grids[f'classifier__max_depth'] = [3, 5]  # Reduced number of values\n",
    "\n",
    "    # Set the classifier in the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stop_words,lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    start_time = time.time()\n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grids, scoring='accuracy', cv=5, n_jobs=-1, verbose=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time = time.time()  # Record the end time\n",
    "    total_training_time = end_time - start_time \n",
    "    # Save the best model using joblib\n",
    "    best_model = grid_search.best_estimator_\n",
    "    model_filename = f\"{joblib_path}{type(classifier).__name__}_best_model.joblib\"\n",
    "    dump(best_model, model_filename)\n",
    "    best_params = grid_search.best_params_\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy_score = grid_search.best_score_\n",
    "    train_accuracy_list_tuned.append({\n",
    "        'Classifier': type(classifier).__name__,\n",
    "        'Train Accuracy': train_accuracy_score,\n",
    "        'Total Training Time (s)': total_training_time\n",
    "    })\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = grid_search.predict(X_test)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    confusion = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # Append results to the list\n",
    "    test_results_list_tuned.append({\n",
    "        'Classifier': type(classifier).__name__,\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1 Score': f1,\n",
    "        'Specificity': specificity,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Best Hyperparameters': best_params\n",
    "    })\n",
    "    \n",
    "    train_predictions = grid_search.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    train_confusion = confusion_matrix(y_train, train_predictions)\n",
    "    train_tn, train_fp, train_fn, train_tp = train_confusion.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_recall = train_tp / (train_tp + train_fn)\n",
    "    train_precision = train_tp / (train_tp + train_fp)\n",
    "    train_specificity = train_tn / (train_tn + train_fp)\n",
    "    train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall)\n",
    "    # Append results to the list\n",
    "    train_results_list_tuned.append({\n",
    "        'Classifier': type(classifier).__name__,\n",
    "        'Accuracy': train_accuracy,\n",
    "        'Recall':train_recall,\n",
    "        'Precision': train_precision,\n",
    "        'F1 Score': train_f1,\n",
    "        'Specificity': train_specificity,\n",
    "        'TP': train_tp,\n",
    "        'TN': train_tn,\n",
    "        'FP': train_fp,\n",
    "        'FN': train_fn\n",
    "    })\n",
    "    \n",
    "    \n",
    "    print(f\"Test Accuracy for {type(classifier).__name__}: {accuracy}\")\n",
    "    print(f\"Training Accuracy Score for {type(classifier).__name__}: {train_accuracy_score}\")\n",
    "    print(f\"Training Accuracy all training data for {type(classifier).__name__}: {train_accuracy}\")\n",
    "    print(f\"Best Parameters for {type(classifier).__name__}: {best_params}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Convert the lists of dictionaries to DataFrames\n",
    "test_results_df_tuned = pd.DataFrame(test_results_list_tuned)\n",
    "train_accuracy_df_tuned = pd.DataFrame(train_accuracy_list_tuned)\n",
    "train_accuracy_df_all_tuned  = pd.DataFrame(train_results_list_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e6a55a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T02:49:30.904478Z",
     "start_time": "2023-12-31T02:49:30.894881Z"
    }
   },
   "outputs": [],
   "source": [
    "test_results_df_tuned.to_csv('TEST_ACCURACY_tuned_DEFINED_HYPERPARAMETER.csv')\n",
    "train_accuracy_df_tuned.to_csv('TRAIN_ACCURACY_tuned_DEFINED_HYPERPARAMETER.csv')\n",
    "train_accuracy_df_all_tuned.to_csv('TRAIN_ACCURACY_OVERALL_tuned_TRAIN_DATA_DEFINED_HYPERPARAMETER.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
